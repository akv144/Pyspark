{"cells":[{"cell_type":"code","source":["\n# Usage of spark object in PySpark shell\nspark.version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85106b2f-5eeb-4498-92ac-883ab96b2d43"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: '3.2.1'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: '3.2.1'"]}}],"execution_count":0},{"cell_type":"code","source":["# Create SparkSession from builder\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\nspark\n\n'''\nmaster() – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either yarn or mesos depends on your cluster setup.\n\nUse local[x] when running in Standalone mode. x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\nappName() – Used to set your application name.\n\ngetOrCreate() – This returns a SparkSession object if already exists, and creates a new one if not exist.\n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f0499a6-c388-4d3e-bc8b-b045dd8ea6b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2995350661662673#setting/sparkui/0804-005325-ui4uu8e/driver-8166429653797571669\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2995350661662673#setting/sparkui/0804-005325-ui4uu8e/driver-8166429653797571669\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["# Create new SparkSession\nspark2 = SparkSession.newSession\nprint(spark2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbf66434-c8b4-4fd6-9ed1-3d850951d23f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<function SparkSession.newSession at 0x7fee6621c280>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<function SparkSession.newSession at 0x7fee6621c280>\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Get Existing SparkSession\nspark3 = SparkSession.builder.getOrCreate\nprint(spark3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"553ab363-5cb3-4c84-912e-bb745bdb08d8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x7fee66376310>>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x7fee66376310>>\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Usage of config()\nspark = SparkSession.builder \\\n      .master(\"local[1]\") \\\n      .appName(\"SparkByExamples.com\") \\\n      .config(\"spark.some.config.option\", \"config-value\") \\\n      .getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9637f52-e926-46e4-babf-253c3330be9c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Enabling Hive to use in Spark -- enableHiveSupport()\nspark = SparkSession.builder \\\n      .master(\"local[1]\") \\\n      .appName(\"SparkByExamples.com\") \\\n      .config(\"spark.sql.warehouse.dir\", \"<path>/spark-warehouse\") \\\n      .enableHiveSupport() \\\n      .getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c598872-59df-444d-ae14-ed9fe754e376"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Set Config\nspark.conf.set(\"spark.executor.memory\", \"5g\")\n# Get a Spark Config\npartions = spark.conf.get(\"spark.sql.shuffle.partitions\")\nprint(partions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdae928b-2f1a-463c-a8c3-1a9154d4bccc"}}},{"cell_type":"code","source":["# Create DataFrame\ndf = spark.createDataFrame(\n    [(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)],[\"language\",\"price\"])\ndf.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d195f87-1d3e-44a4-8803-824268680416"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+\n|language|price|\n+--------+-----+\n|   Scala|25000|\n|   Spark|35000|\n|     PHP|21000|\n+--------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+\n|language|price|\n+--------+-----+\n|   Scala|25000|\n|   Spark|35000|\n|     PHP|21000|\n+--------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Spark SQL\ndf.createOrReplaceTempView(\"sample_table\")\ndf2 = spark.sql(\"SELECT * FROM sample_table\")\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb146e0a-2ccd-40bb-80d3-62edc904c1b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+\n|language|price|\n+--------+-----+\n|   Scala|25000|\n|   Spark|35000|\n|     PHP|21000|\n+--------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+\n|language|price|\n+--------+-----+\n|   Scala|25000|\n|   Spark|35000|\n|     PHP|21000|\n+--------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Create Hive table & query it.  \nspark.table(\"sample_table\").write.saveAsTable(\"sample_hive_table\")\ndf3 = spark.sql(\"SELECT * FROM sample_hive_table\")\ndf3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"082ffef7-6aac-49b1-aed3-86e3ca5fc095"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+\n|language|price|\n+--------+-----+\n|   Scala|25000|\n|   Spark|35000|\n|     PHP|21000|\n+--------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+\n|language|price|\n+--------+-----+\n|   Scala|25000|\n|   Spark|35000|\n|     PHP|21000|\n+--------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Get metadata from the Catalog\n# List databases\ndbs = spark.catalog.listDatabases()\nprint(dbs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5331c7d5-e5f7-4c36-a9b0-8b9dac410541"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Database(name='default', description='Default Hive database', locationUri='dbfs:/user/hive/warehouse')]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Database(name='default', description='Default Hive database', locationUri='dbfs:/user/hive/warehouse')]\n"]}}],"execution_count":0},{"cell_type":"code","source":["# List Tables\ntbls = spark.catalog.listTables()\nprint(tbls)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"987ce60f-097d-4447-9904-c8fb6a1b6d78"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Table(name='sample_hive_table', database='default', description=None, tableType='MANAGED', isTemporary=False), Table(name='sample_table', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Table(name='sample_hive_table', database='default', description=None, tableType='MANAGED', isTemporary=False), Table(name='sample_table', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["8. SparkSession Commonly Used Methods\n\nversion() – Returns the Spark version where your application is running, probably the Spark version your cluster is configured with.\n\ncreateDataFrame() – This creates a DataFrame from a collection and an RDD\n\ngetActiveSession() – returns an active Spark session.\n\nread() – Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro, and more file formats into DataFrame.\n\nreadStream() – Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to read streaming data into DataFrame.\n\nsparkContext() – Returns a SparkContext.\n\nsql() – Returns a DataFrame after executing the SQL mentioned.\n\nsqlContext() – Returns SQLContext.\n\nstop() – Stop the current SparkContext.\n\ntable() – Returns a DataFrame of a table or view.\n\nudf() – Creates a PySpark UDF to use it on DataFrame, Dataset, and SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef6a08ef-2fb8-437c-8ce2-d327e76c59ea"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"How to create SparkSession","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2893520827234188}},"nbformat":4,"nbformat_minor":0}
